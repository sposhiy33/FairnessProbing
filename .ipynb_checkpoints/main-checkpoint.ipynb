{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e787c3b-7781-4508-bab9-03eb146e82eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from torch.utils.data.sampler import (SubsetRandomSampler,\n",
    "                                      RandomSampler)\n",
    "from torchvision import datasets, transforms\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52b83fc9-8993-493a-9914-f1d43f65d1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c5f7c-57aa-42f9-9f57-ccbde0709a39",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085faed3-fa34-4c0f-b0c8-39c275fb30ea",
   "metadata": {},
   "source": [
    "### COMPAS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5abd01d7-45ef-4eda-8c49-998bd5f8f94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compas = pd.read_csv(\"COMPAS_ProPublica/propublicaCompassRecividism_data_fairml.csv/propublica_data_for_fairml.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286bcf6c-d01a-4a3e-a65a-84b3e7e858e8",
   "metadata": {},
   "source": [
    "### ADULT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73217531-fd03-4bfe-ba94-a6d8f71ace76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size Before pruning:  43832\n",
      "Dataset size after pruning:  43832\n",
      "We eliminated  0  datapoints\n",
      "43832\n",
      "43832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14724/952712682.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[i].replace('nan', np.nan, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "adult = pd.read_csv(\"UCIAdult/adult.csv\",\n",
    "                   dtype={0:int, 1:str, 2:int, 3:str, 4:int, 5: str, 6:str , 7:str ,8:str ,9: str, 10:int, 11:int, 12:int, 13:str,14: str})\n",
    "\n",
    "# Keep only United States samples, due to class imbalance\n",
    "adult = adult[adult[\"native-country\"] == 'United-States']\n",
    "\n",
    "# Then drop Column\n",
    "adult = adult.drop(['native-country'], axis=1)\n",
    "\n",
    "# get rid of NAN vales\n",
    "full_data = adult\n",
    "\n",
    "str_list=[]\n",
    "\n",
    "for data in [full_data]:\n",
    "    for colname, colvalue in data.items(): \n",
    "        if type(colvalue[1]) == str:\n",
    "            str_list.append(colname) \n",
    "num_list = data.columns.difference(str_list)\n",
    "\n",
    "full_size = full_data.shape[0]\n",
    "print('Dataset size Before pruning: ', full_size)\n",
    "for data in [full_data]:\n",
    "    for i in full_data:\n",
    "        data[i].replace('nan', np.nan, inplace=True)\n",
    "    data.dropna(inplace=True)\n",
    "real_size = full_data.shape[0]\n",
    "print('Dataset size after pruning: ', real_size)\n",
    "print('We eliminated ', (full_size-real_size), ' datapoints')\n",
    "\n",
    "### make prediction labels ### \n",
    "full_labels = full_data['income'].copy()\n",
    "print(full_labels.shape[0])\n",
    "\n",
    "full_data = full_data.drop(['income'], axis=1)\n",
    "print(full_data.shape[0])\n",
    "\n",
    "# Label Encode Labels\n",
    "label_encoder = LabelEncoder()\n",
    "full_labels = label_encoder.fit_transform(full_labels)\n",
    "\n",
    "\n",
    "### Deal with categorical data ###\n",
    "\n",
    "cat_data = full_data.select_dtypes(include=['object']).copy()\n",
    "other_data = full_data.select_dtypes(include=['int']).copy()\n",
    "\n",
    "newcat_data = pd.get_dummies(cat_data, columns = ['workclass', 'education',\n",
    "       'marital-status', 'occupation', 'relationship', 'race'], dtype=int)\n",
    "\n",
    "full_data = pd.concat([other_data, newcat_data], axis=1)\n",
    "adult = full_data\n",
    "adult_labels = full_labels\n",
    "\n",
    "# one_hot = F.one_hot(torch.Tensor(adult_labels).to(torch.int).long(), 2)\n",
    "# adult_labels = one_hot\n",
    "# adult_labels = adult_labels.numpy()\n",
    "\n",
    "adult = adult.drop(['gender'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1453ec61-d8a8-4cd2-b90d-34bf85a5e00a",
   "metadata": {},
   "source": [
    "### DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f713fbe-d11b-4994-a31b-3643c1992561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c65df361-a9f6-41b4-9b2f-f1a59da5dd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):    \n",
    "    def __init__(self, dataframe, targets, transform=None):\n",
    "        self.data_frame = dataframe\n",
    "        self.targets = targets\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        m_data = self.data_frame.iloc[idx, :].values\n",
    "        m_target = self.targets[idx]\n",
    "        m_data = m_data.astype('float')\n",
    "        m_target = m_target.astype('float')\n",
    "        sample = {'target': np.array([m_target]), 'data': np.array(m_data)}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aba84e6-49d6-44e0-bd41-6e11fc4c34e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "adultDataset = MyDataset(adult, adult_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c6a7de-e310-4ea2-b10a-5ac1846fd27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(adultDataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "bs = 1028\n",
    "batch_size_eval = 128\n",
    "\n",
    "# Split dataset into train and Test sets\n",
    "adult_train_loader = DataLoader(\n",
    "    adultDataset,\n",
    "    batch_size=bs,\n",
    "    sampler=SubsetRandomSampler(indices[:30001]),\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "adult_valid_loader = DataLoader(\n",
    "    adultDataset,\n",
    "    batch_size=batch_size_eval,\n",
    "    sampler=SubsetRandomSampler(indices[30001:38001]),\n",
    "    num_workers=1,\n",
    ")\n",
    "\n",
    "adult_test_loader = DataLoader(\n",
    "    adultDataset,\n",
    "    batch_size=batch_size_eval,\n",
    "    sampler=SubsetRandomSampler(indices[38001:]),\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cba3e8a-b49c-4106-81fa-85517e07b562",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4951d454-6d52-4e94-911b-e74385d6e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wide(nn.Module):\n",
    "    def __init__(self, num_features_in, num_classes):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(num_features_in, 180)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.hidden1 = nn.Linear(180, 90)\n",
    "        self.output = nn.Linear(90, num_classes)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.hidden(x))\n",
    "        x = self.relu(self.hidden1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bdfde30-c452-4eea-b91e-5332efc97f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep(nn.Module):\n",
    "    def __init__(self,num_features_in, num_classes):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(num_features_in, 60)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(60, 60)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(60, 60)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.output = nn.Linear(60, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.layer1(x))\n",
    "        x = self.act2(self.layer2(x))\n",
    "        x = self.act3(self.layer3(x))\n",
    "        x = self.sigmoid(self.output(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ed111-09d1-45f3-baba-5fb43c08f758",
   "metadata": {},
   "source": [
    "## ENGINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37c027b9-8a5b-44e9-a943-6c09c4a72fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    for data in train_loader:\n",
    "        inputs = data[\"data\"]\n",
    "        target = data[\"target\"]\n",
    "        inputs, target = inputs.to(torch.float32), target.to(torch.float32)\n",
    "        target = target.type(torch.LongTensor)\n",
    "        inputs, target = inputs.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs)\n",
    "\n",
    "        target = torch.squeeze(target, dim=1)\n",
    "\n",
    "        loss = loss_fn(output, target)\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ###\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    test_size = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      \n",
    "        for data in test_loader:\n",
    "            inputs = data[\"data\"]\n",
    "            target = data[\"target\"]\n",
    "            inputs, target = inputs.to(torch.float32), target.to(torch.float32)\n",
    "            target = target.type(torch.LongTensor)\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            \n",
    "            output = model(inputs)\n",
    "            test_size += len(inputs)\n",
    "\n",
    "            target = torch.squeeze(target, dim=1)\n",
    "            \n",
    "            test_loss += test_loss_fn(output, target).item() \n",
    "            # indicies of max pred\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= test_size\n",
    "    accuracy = correct / test_size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, test_size,\n",
    "        100. * accuracy))\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a460a4-55dd-4157-9461-8a3d3595be45",
   "metadata": {},
   "source": [
    "## Vanilla Accuracy Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5cbb46a-8a07-4992-8b1c-18d3d168d84e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training beginning...\n",
      "Epoch  1 :\n",
      "\n",
      "Test set: Average loss: 287.2973, Accuracy: 6048/8000 (76%)\n",
      "\n",
      "Epoch  2 :\n",
      "\n",
      "Test set: Average loss: 147.8070, Accuracy: 6085/8000 (76%)\n",
      "\n",
      "Epoch  3 :\n",
      "\n",
      "Test set: Average loss: 95.1159, Accuracy: 6126/8000 (77%)\n",
      "\n",
      "Epoch  4 :\n",
      "\n",
      "Test set: Average loss: 54.0652, Accuracy: 6238/8000 (78%)\n",
      "\n",
      "Epoch  5 :\n",
      "\n",
      "Test set: Average loss: 23.9191, Accuracy: 6302/8000 (79%)\n",
      "\n",
      "Epoch  6 :\n",
      "\n",
      "Test set: Average loss: 4.5424, Accuracy: 6269/8000 (78%)\n",
      "\n",
      "Epoch  7 :\n",
      "\n",
      "Test set: Average loss: 0.7455, Accuracy: 6272/8000 (78%)\n",
      "\n",
      "Epoch  8 :\n",
      "\n",
      "Test set: Average loss: 0.6880, Accuracy: 6326/8000 (79%)\n",
      "\n",
      "Epoch  9 :\n",
      "\n",
      "Test set: Average loss: 0.6493, Accuracy: 6333/8000 (79%)\n",
      "\n",
      "Epoch  10 :\n",
      "\n",
      "Test set: Average loss: 0.6296, Accuracy: 6312/8000 (79%)\n",
      "\n",
      "Epoch  11 :\n",
      "\n",
      "Test set: Average loss: 0.6207, Accuracy: 6327/8000 (79%)\n",
      "\n",
      "Epoch  12 :\n",
      "\n",
      "Test set: Average loss: 0.6226, Accuracy: 6300/8000 (79%)\n",
      "\n",
      "Epoch  13 :\n",
      "\n",
      "Test set: Average loss: 0.6009, Accuracy: 6313/8000 (79%)\n",
      "\n",
      "Epoch  14 :\n",
      "\n",
      "Test set: Average loss: 0.6018, Accuracy: 6320/8000 (79%)\n",
      "\n",
      "Epoch  15 :\n",
      "\n",
      "Test set: Average loss: 0.5998, Accuracy: 6315/8000 (79%)\n",
      "\n",
      "Epoch  16 :\n",
      "\n",
      "Test set: Average loss: 0.6097, Accuracy: 6305/8000 (79%)\n",
      "\n",
      "Epoch  17 :\n",
      "\n",
      "Test set: Average loss: 0.6074, Accuracy: 6306/8000 (79%)\n",
      "\n",
      "Epoch  18 :\n",
      "\n",
      "Test set: Average loss: 0.5971, Accuracy: 6301/8000 (79%)\n",
      "\n",
      "Epoch  19 :\n",
      "\n",
      "Test set: Average loss: 0.5841, Accuracy: 6329/8000 (79%)\n",
      "\n",
      "Epoch  20 :\n",
      "\n",
      "Test set: Average loss: 0.5916, Accuracy: 6300/8000 (79%)\n",
      "\n",
      "Epoch  21 :\n",
      "\n",
      "Test set: Average loss: 0.5885, Accuracy: 6308/8000 (79%)\n",
      "\n",
      "Epoch  22 :\n",
      "\n",
      "Test set: Average loss: 0.5914, Accuracy: 6308/8000 (79%)\n",
      "\n",
      "Epoch  23 :\n",
      "\n",
      "Test set: Average loss: 0.5951, Accuracy: 6308/8000 (79%)\n",
      "\n",
      "Epoch  24 :\n",
      "\n",
      "Test set: Average loss: 0.5823, Accuracy: 6304/8000 (79%)\n",
      "\n",
      "Epoch  25 :\n",
      "\n",
      "Test set: Average loss: 0.5836, Accuracy: 6300/8000 (79%)\n",
      "\n",
      "Epoch  26 :\n",
      "\n",
      "Test set: Average loss: 0.5947, Accuracy: 6307/8000 (79%)\n",
      "\n",
      "Epoch  27 :\n",
      "\n",
      "Test set: Average loss: 0.5869, Accuracy: 6309/8000 (79%)\n",
      "\n",
      "Epoch  28 :\n",
      "\n",
      "Test set: Average loss: 0.5918, Accuracy: 6286/8000 (79%)\n",
      "\n",
      "Epoch  29 :\n",
      "\n",
      "Test set: Average loss: 0.5823, Accuracy: 6309/8000 (79%)\n",
      "\n",
      "Epoch  30 :\n",
      "\n",
      "Test set: Average loss: 0.5783, Accuracy: 6300/8000 (79%)\n",
      "\n",
      "Epoch  31 :\n",
      "\n",
      "Test set: Average loss: 0.5882, Accuracy: 6291/8000 (79%)\n",
      "\n",
      "Epoch  32 :\n",
      "\n",
      "Test set: Average loss: 0.5807, Accuracy: 6306/8000 (79%)\n",
      "\n",
      "Epoch  33 :\n",
      "\n",
      "Test set: Average loss: 0.5886, Accuracy: 6304/8000 (79%)\n",
      "\n",
      "Epoch  34 :\n",
      "\n",
      "Test set: Average loss: 0.5908, Accuracy: 6300/8000 (79%)\n",
      "\n",
      "Epoch  35 :\n",
      "\n",
      "Test set: Average loss: 0.5780, Accuracy: 6308/8000 (79%)\n",
      "\n",
      "Epoch  36 :\n",
      "\n",
      "Test set: Average loss: 0.5747, Accuracy: 6298/8000 (79%)\n",
      "\n",
      "Epoch  37 :\n",
      "\n",
      "Test set: Average loss: 0.5989, Accuracy: 6295/8000 (79%)\n",
      "\n",
      "Epoch  38 :\n",
      "\n",
      "Test set: Average loss: 0.5739, Accuracy: 6304/8000 (79%)\n",
      "\n",
      "Epoch  39 :\n",
      "\n",
      "Test set: Average loss: 0.5788, Accuracy: 6296/8000 (79%)\n",
      "\n",
      "Epoch  40 :\n",
      "\n",
      "Test set: Average loss: 0.5847, Accuracy: 6282/8000 (79%)\n",
      "\n",
      "Epoch  41 :\n",
      "\n",
      "Test set: Average loss: 0.5879, Accuracy: 6283/8000 (79%)\n",
      "\n",
      "Epoch  42 :\n",
      "\n",
      "Test set: Average loss: 0.6092, Accuracy: 6245/8000 (78%)\n",
      "\n",
      "Epoch  43 :\n",
      "\n",
      "Test set: Average loss: 0.5869, Accuracy: 6286/8000 (79%)\n",
      "\n",
      "Epoch  44 :\n",
      "\n",
      "Test set: Average loss: 0.5705, Accuracy: 6312/8000 (79%)\n",
      "\n",
      "Epoch  45 :\n",
      "\n",
      "Test set: Average loss: 0.5888, Accuracy: 6269/8000 (78%)\n",
      "\n",
      "Epoch  46 :\n",
      "\n",
      "Test set: Average loss: 0.5809, Accuracy: 6250/8000 (78%)\n",
      "\n",
      "Epoch  47 :\n",
      "\n",
      "Test set: Average loss: 0.5695, Accuracy: 6305/8000 (79%)\n",
      "\n",
      "Epoch  48 :\n",
      "\n",
      "Test set: Average loss: 0.5735, Accuracy: 6308/8000 (79%)\n",
      "\n",
      "Epoch  49 :\n",
      "\n",
      "Test set: Average loss: 0.5995, Accuracy: 6265/8000 (78%)\n",
      "\n",
      "Epoch  50 :\n",
      "\n",
      "Test set: Average loss: 0.5679, Accuracy: 6313/8000 (79%)\n",
      "\n",
      "Training on 50 epochs done in  132.40625262260437  seconds\n",
      "\n",
      "Test set: Average loss: 0.5449, Accuracy: 4555/5831 (78%)\n",
      "\n",
      "0.5449340603114851 0.7811696107014234\n"
     ]
    }
   ],
   "source": [
    "num_features = 64\n",
    "model = Wide(num_features, 2).to(device)\n",
    "test_accuracy = []\n",
    "train_loss = []\n",
    "nbr_epochs = 50\n",
    "lr = 0.0001\n",
    "weight_decay = 0\n",
    "\n",
    "\n",
    "\n",
    "# Surrogate loss used for training\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test_loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr ,weight_decay=weight_decay)\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "print('Training beginning...')\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, nbr_epochs+1):\n",
    "    print('Epoch ', epoch, ':')\n",
    "    train(model, adult_train_loader, optimizer, epoch)\n",
    "    loss, acc = test(model, adult_valid_loader)\n",
    "    \n",
    "    # save results every epoch\n",
    "    test_accuracy.append(acc)\n",
    "    train_loss.append(loss)\n",
    "    \n",
    "end_time = time.time()\n",
    "print('Training on ' + str(nbr_epochs) + ' epochs done in ', str(end_time-start_time),' seconds')\n",
    "\n",
    "test_loss, test_acc = test(model, adult_test_loader)\n",
    "print(test_loss, test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae7402fb-601f-4de4-9e2f-eeb178160980",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()}, \"modelDicts/vanilla.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb3a58-1673-4eed-8090-7777da1db550",
   "metadata": {},
   "source": [
    "## Demographic Parity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ca65d06-8581-444c-ac14-079687b7ded9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (1669807239.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    def demoParityLoss(outputs, targets, samples, parityCoef):\u001b[0m\n\u001b[0m                                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "## define the combination of accuracy and demographic parity regularization\n",
    "def demoParityLoss(outputs, targets, samples, parityCoef):\n",
    "    ## accuracy\n",
    "    entropyLoss = loss_fn(outputs, targets)\n",
    "\n",
    "    loss = entropyLoss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e5a34-9ba9-41ab-87f2-5f740eb9542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "demModel = Wide(num_features, 2).to(device)\n",
    "\n",
    "# Surrogate loss used for training\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "test_loss_fn = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr,weight_decay=weight_decay)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=lr ,weight_decay=weight_decay)\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "print('Training beginning...')\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, nbr_epochs+1):\n",
    "    print('Epoch ', epoch, ':')\n",
    "    train(demModel, adult_train_loader, optimizer, epoch)\n",
    "    loss, acc = test(demModel, adult_valid_loader)\n",
    "    \n",
    "    # save results every epoch\n",
    "    test_accuracy.append(acc)\n",
    "    train_loss.append(loss)\n",
    "    \n",
    "end_time = time.time()\n",
    "print('Training on ' + str(nbr_epochs) + ' epochs done in ', str(end_time-start_time),' seconds')\n",
    "\n",
    "test_loss, test_acc = test(model, adult_test_loader)\n",
    "print(test_loss, test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
